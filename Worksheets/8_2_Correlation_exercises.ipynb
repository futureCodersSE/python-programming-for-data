{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "8.2. Correlation exercises.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/futureCodersSE/python-programming-for-data/blob/main/Worksheets/8_2_Correlation_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7raIDLBmCVNJ"
      },
      "source": [
        "# Describing the data relatively\n",
        "---\n",
        "This worksheet has a set of exercises for practising using the linregress function.  The function will run a linear regression algorithm (or model) to produce data that can be used for predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVgw-x5VCVNN"
      },
      "source": [
        "### Exercise 1 - Clean the data \n",
        "---\n",
        "Using the positive psychology dataset - \"https://github.com/lilaceri/Working-with-data-/blob/b157a2feceb7709cf82426932385706d65446270/Data%20Sets%20for%20code%20divisio/Positive_Psychology_2017.csv?raw=true\"\n",
        "* Read the data and display info \n",
        "* Visually check the summary to see which columns have null values\n",
        "* Remove columns with a significant number of null values\n",
        "\n",
        "**Expected Output**\n",
        "```\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 44 entries, 0 to 43\n",
        "Data columns (total 13 columns):\n",
        " #   Column       Non-Null Count  Dtype  \n",
        "---  ------       --------------  -----  \n",
        " 0   Age          44 non-null     int64  \n",
        " 1   English      44 non-null     object \n",
        " 2   sex          44 non-null     object \n",
        " 3   origin       44 non-null     object \n",
        " 4   Ukresidence  44 non-null     float64\n",
        " 5   MAAS         44 non-null     float64\n",
        " 6   Resilliance  44 non-null     int64  \n",
        " 7   Wellbeing    44 non-null     int64  \n",
        " 8   Stress       44 non-null     int64  \n",
        " 9   selfesteem   44 non-null     int64  \n",
        " 10  LoC          44 non-null     int64  \n",
        " 11  sleep        44 non-null     int64  \n",
        " 12  Year         44 non-null     int64  \n",
        "dtypes: float64(2), int64(8), object(3)\n",
        "memory usage: 4.6+ KB\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U5CsP0ECVNO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5_G1iWYCVNP"
      },
      "source": [
        "### Exercise 2 - Check for outliers \n",
        "---\n",
        "We briefly mentioned outliers in the R and stats presentation. An outlier is an abnormal value in the data that is either extremely high or low compared to the rest of the data. Outliers can skew results. \n",
        "\n",
        "A value is considered to be an outlier if it is any of:\n",
        "*  greater than `Q3 + 1.5 * interquartile_range`\n",
        "*  less than `Q1 - 1.5 * interquartile_range`\n",
        "*  more than 2 standard deviations from the mean\n",
        "\n",
        "Q3 = value at 75% of data, Q1 = value at 25% of the data, IQR is the interquartile range, the difference between Q3 and Q1.  \n",
        "\n",
        "We can check for outliers using a Box plot, in which:\n",
        "\n",
        "*  the upper line on the boxplots arm (whisker) is `Q3 + 1.5 * IQR`\n",
        "*  the lower line on the boxplots arm (whisker) is `Q1 - 1.5 * IQR`\n",
        "*  the middle line of the box is the median\n",
        "*  the top of the box is Q3\n",
        "*  the bottom of the box is Q1\n",
        "*  outliers are shown as circles or dots, either above or below the whiskers\n",
        "\n",
        "1. Using either matplotlib or Seaborn, create a boxplot of `Wellbeing` and a separate boxplot of `selfesteem`\n",
        "2. use `plt.show()` to separate the graphs \n",
        "3. Can you see any outliers? Are they high or low?\n",
        "\n",
        "**Expected Output**\n",
        "\n",
        "https://docs.google.com/presentation/d/e/2PACX-1vQBji5MrvtdeXCtP2PJzhPLKqXXuLMYjy4nCIzXpJLoye38IzetN5amZd6pU9e4io3bTUvE6Slg_hIk/pub?start=false&loop=false&delayms=3000\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLT4pozqCVNQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VCT8LnOCVNQ"
      },
      "source": [
        "### Exercise 3 - Removing outliers\n",
        "---\n",
        "Create a function called `remove_outliers(df, column)` to remove outliers from a given column in a given dataframe.  Use the function to remove outliers from the `Wellbeing` column.  Then use the function to remove the outliers from the `selfesteem` columns. \n",
        "\n",
        "The function will:\n",
        "\n",
        "1. Store the values for Q1 and Q3 in 2 separate variables   *Hint: you can use `.quantile(0.75)` and `.quantile(0.25)` to get Q3 and Q1 respectively*  \n",
        "2. Calculate the interquartile range(IQR) using `Q3` - `Q1`  \n",
        "3. Create a variable which will store the value for the `upper_limit` (`Q3 + 1.5 * IQR`)  \n",
        "4. Create another variable which assigns the value for the `lower_limit` (`Q1  - 1.5 * IQR`)  \n",
        "5. Filter all rows where values are NOT outliers into a new dataframe called `df_normal`\n",
        "6. Return `df_normal`\n",
        "\n",
        "Run the function twice, once for each column.\n",
        "Show the info for the resulting dataframe\n",
        "\n",
        "**Test output**:  \n",
        "```\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 41 entries, 0 to 43\n",
        "Data columns (total 13 columns):\n",
        " #   Column       Non-Null Count  Dtype  \n",
        "---  ------       --------------  -----  \n",
        " 0   Age          41 non-null     int64  \n",
        " 1   English      41 non-null     object \n",
        " 2   sex          41 non-null     object \n",
        " 3   origin       41 non-null     object \n",
        " 4   Ukresidence  41 non-null     float64\n",
        " 5   MAAS         41 non-null     float64\n",
        " 6   Resilliance  41 non-null     int64  \n",
        " 7   Wellbeing    41 non-null     int64  \n",
        " 8   Stress       41 non-null     int64  \n",
        " 9   selfesteem   41 non-null     int64  \n",
        " 10  LoC          41 non-null     int64  \n",
        " 11  sleep        41 non-null     int64  \n",
        " 12  Year         41 non-null     int64  \n",
        "dtypes: float64(2), int64(8), object(3)\n",
        "memory usage: 4.5+ KB\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFvT_TtjCVNR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRs6sfE-CVNR"
      },
      "source": [
        "### Exercise 4 - Checking for normality\n",
        "---\n",
        "\n",
        "Linear regressions assume that the data is normally distributed (Gaussian) and therefore has the bell curved shape and a similar mean and median (this would be true if the mean was centred like it is in Gaussian data). \n",
        "\n",
        "1. Use Seaborn's **distplot** to check the shape of the `Wellbeing` and `selfesteem` columns.  Do they look normally distributed (Gaussian)?\n",
        "\n",
        "`sns.distplot(dataframe['column'])`\n",
        "\n",
        "*Hint: if you add, `bins= ...` as a parameter, you can set the number of bins (bars) in your chart*\n",
        "\n",
        "2. Compare the mean and median of `Wellbeing` - are they similar? \n",
        "3. Compare the mean and median of `selfesteem` - are they similar?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOxGYfg-CVNS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nIUQF5TCVNT"
      },
      "source": [
        "### Exercise 5 - Check which variables are most highly correlated\n",
        "---\n",
        "Using the `dataframe.corr()` function and Seaborn's heatmap, create a correlation heatmap matrix to check which variables are most highly correlated.\n",
        "\n",
        "A correlation matrix finds the correlation coefficient between every variable pair combination in a set of variables.  A correlation heatmap shows the degree of correlation between variables using a colour scale.\n",
        "\n",
        "*The matrix and heatmap will always show the correlation coefficients between each variable and itself, which will have a correlation coefficient of 1.*\n",
        "  \n",
        "Values closest to 1 mean the variables are positively correlated with each other with 1 meaning 100% correlated\n",
        "\n",
        "\n",
        "Values close to -1 mean the variables are negatively correlated with each other with -1 meaning 100% negatively correlated\n",
        "\n",
        "1. create the correlation matrix assigning it to a variable called **correlation_matrix** using the .corr() function and rounding to 2 decimal places:\n",
        "\n",
        "  `correlation_matrix = dataframe.corr().round(2)`\n",
        "\n",
        "*This will create a matrix similar to the linear regression matrix created in the numpy worksheet, but with more than two variables.*\n",
        "\n",
        "2. create a heatmap of the correlation matrix using:\n",
        "\n",
        " `sns.heatmap(data=data_variable, annot=True)`\n",
        "\n",
        "*Hint: Use `annot = True` to print the correlation values inside the square*\n",
        "\n",
        "3. Which variables are the most highly correlated with each other (closest to 1 or -1)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyYYASZvCVNT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZgZHqbHCVNU"
      },
      "source": [
        "### Exercise 6 - Linear regression \n",
        "---\n",
        "Using the `linregress` function, perform a linear regression on the Wellbeing and selfesteem columns   \n",
        "To do this:\n",
        "\n",
        "1. `from scipy.stats import linregress`\n",
        "2. create a variable 'x' which stores the Wellbeing column\n",
        "3. create a variable 'y' which stores the selfesteem column\n",
        "4. create a variable called regression and assign the result of running the linregress function with x, y as its parameters ( linregress(x,y) )\n",
        "5. display the regression\n",
        "6. display the slope (regression.slope)\n",
        "7. display the y-intercept\n",
        "8. display the r^2 value (rvalue**2)\n",
        "9. display the pvalue \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SpVEyHZCVNU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ordhi_WUCVNU"
      },
      "source": [
        "### Exercise 7 - understanding the output \n",
        "---\n",
        "The r^2 value (rvalue**2) helps us understand how much our independent variable (x) is predicting our dependent variable (y). The closer to 1 the rvalue is the more  the change in Y is explained by X. So an rvalue of 0.7 means that 70% of Y's variance can be explained by X. \n",
        "\n",
        "Our pvalue shows how significant our model is, if the pvalue is < 0.05 then the model is significant. \n",
        "\n",
        "On this basis, write below the findings of the above regression.\n",
        "* is the model significant?\n",
        "* how much is the change in y explained by x? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVCxB73eCVNV"
      },
      "source": [
        "**Write about the findings here**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TcYPrOCVNV"
      },
      "source": [
        "### Exercise 8 - Linear regression with other variables \n",
        "---\n",
        "\n",
        "Referring back to the correlation heatmap..  \n",
        "*  Repeat Exercise 6 but with the variables that were the most highly correlated according to the heatmap.   \n",
        "* write a comment comparing the results of this regression with the one you created in exercise 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHJi2r8zCVNV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaFhxdjEzKNL"
      },
      "source": [
        "**Comment here**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvvgY68hCVNW"
      },
      "source": [
        "### Exercise 9 - Plotting a line of best fit \n",
        "---\n",
        "To create a line of best fit we use y = slope\\*x + intercept. \n",
        "\n",
        "Using matplotlib (dont forget to import it):\n",
        "\n",
        "* create a scatter graph between Wellbeing and selfesteem \n",
        "* plot a line of best fit using the results in exercise 6  (y = slope * x + intercept)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVT8QqYCCVNW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KcUIkXHCVNW"
      },
      "source": [
        "### Exercise 10 - using Seaborn to plot a regression line \n",
        "---\n",
        "\n",
        "Use Seaborn's `regplot` function to create a scatter graph with line of best fit of the variables you used in Exercise 8.\n",
        "\n",
        "* Compare the 'Wellbeing' and 'selfesteem' graph to the graph you created in Exercise 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xVsVD4GCVNX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
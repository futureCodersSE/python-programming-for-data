{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/futureCodersSE/python-programming-for-data/blob/main/1_Data_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "socyySfaK8_B"
      },
      "source": [
        "# Data retrieval\n",
        "---\n",
        "\n",
        "Each of the code cells below contains code that is an example of how data can be retrieved from a range of sources.\n",
        "\n",
        "### Read the text and run the code to see what it does.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ewU932LIO-"
      },
      "source": [
        "## Scraping data from a web page\n",
        "---\n",
        "\n",
        "The code below reads all the data tables from the Wikipedia page on Glasgow.  The 8th table on the page shows population data over a period of centuries.\n",
        "\n",
        "The code reads the data from the page into a list of dataframes.  The index, eg[7] is used to access the 8th table in the list.  \n",
        "\n",
        "1.  Open the link to have a look at the [Glasgow Wikipedia](https://en.wikipedia.org/wiki/Glasgow#Climate) page\n",
        "2.  Run the code.\n",
        "3.  Change the index to see other data tables\n",
        "4.  Add the line\n",
        "```\n",
        "print(len(datatables))\n",
        "```\n",
        "to show how many tables were one the page and so are in the list.\n",
        "\n",
        "## TAKEAWAY:\n",
        "Take a look at a number of the data tables.  They can look messy. The job of the programmer is to write code that will tidy the tables up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAV9q4J3t-zy"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_web_data():\n",
        "  datatables = pd.read_html('https://en.wikipedia.org/wiki/Glasgow#Climate')\n",
        "  #  change the index in [] to look at other tables, add the line print(len(datatables)) to see how many tables there are\n",
        "  df = datatables[7]  #Glasgow population data\n",
        "  return df\n",
        "\n",
        "# run and test the get_data() function, test visually - does it match the data on the web page\n",
        "population_data = get_web_data()\n",
        "display(population_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9m4HHb3Loxe"
      },
      "source": [
        "## From a csv file hosted on Github.com\n",
        "---\n",
        "\n",
        "Data has often already been tidied up and organised into table form.  It is often stored as Comma Separated Values (csv).  This is a formatted text file, which is small and so quick to transfer, especially over the internet.\n",
        "\n",
        "The code below reads a data table stored in a Comma Separated Values file (this is a text file containing rows of data with each column within the row separated from the next column by a comma).  \n",
        "\n",
        "(**Note**: If you were using Jupyter Notebooks on your device, the url could be replaced with the path to the CSV file)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mVk6FwZ-XZu"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_csv_data():\n",
        "  url = \"https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/Paisley-Weather-Data.csv\"\n",
        "  df = pd.read_csv(url)\n",
        "  return df\n",
        "\n",
        "weather_data = get_csv_data()\n",
        "display(weather_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVBfyjCQMVvA"
      },
      "source": [
        "## From an Excel file hosted on Github.com\n",
        "---\n",
        "\n",
        "The code below reads the data table from a sheet in an Excel file.  \n",
        "\n",
        "Excel spreadsheets often have more than one sheet.  If you don't specify a sheet then it will assume that you want to read the data from the first sheet in the Excel workbook (sheet_name = 0).  If you don't know the sheet name but know it is the second sheet, you can use sheet_name = 1, or 2 for the third sheet, etc.\n",
        "\n",
        "The Excel file is readable ONLY if it in its raw format (which is not the format we normally see it in). This is the [original file](https://docs.google.com/spreadsheets/d/1JnGkdYpYdr1hsr_ALCPBduxljQxoF1FK/edit?usp=share_link&ouid=109124845535182996296&rtpof=true&sd=true) in the form we can read.  Have a look at it so you know what you are expecting to see.\n",
        "\n",
        "(**Again**: If you were using Jupyter Notebooks on your device, the url could be replaced with the path to the Excel file)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Att2-4UtBAhW"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_excel_data():\n",
        "  url = \"https://github.com/futureCodersSE/working-with-data/blob/main/Data%20sets/public_use-talent-migration.xlsx?raw=true\"\n",
        "  df = pd.read_excel(url,sheet_name=\"Industry Migration\")\n",
        "  return df\n",
        "\n",
        "migration_df = get_excel_data()\n",
        "display(migration_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NQA3HFJOT8X"
      },
      "source": [
        "## From an API which delivers the data in JSON format\n",
        "---\n",
        "\n",
        "The code below requests the data from a URL.  This is a bit more tricky than the other ways to get the data as how you access the data will depend on how it is organised.\n",
        "\n",
        "In general, the data will be retrieved as a dictionary (not a table), which will contain a record called 'data' in which the actual data is stored.  In the example, the data has been taken from the 'data' record  and is stored in json_data. \n",
        "\n",
        "**Try these to help understand the data**:\n",
        "\n",
        "1.  The code below gets the data from the URL and stores it in a variable called **json_data**.  Run the code to see what the original data looks like.\n",
        "\n",
        "2.  ```json_data``` is a list of records but it only has one record in the list.  **data_table** is the first record in the ```json_data``` list.   \n",
        " * comment out the line ```print(json_data)``` \n",
        " * un-comment the line that assigns data_table the value json_data[0]\n",
        " * un-comment the line that will print data table.\n",
        "\n",
        "3.  In this example, data_table has three keys, 'to', 'from' and 'regions'.  Take a look at the regions data on its own.  \n",
        "  * change the line ```print(data_table)``` to print just the regions part of it: ```print(data_table['regions')```\n",
        "\n",
        "4. The 'regions' value is the data we want to use in our dataframe, so the rest of the code normalizes this json data into a pandas dataframe (df), which you can see as the output.  To see this:  \n",
        "  *  comment out the line  ```print(data_table['regions')```  \n",
        "  *  un-comment the rest of the code to see what the data looks like \n",
        "\n",
        "Each API is likely to deliver its data in a different format and so you will need to be confident to read the documentation and to inspect the data to see what keys and indexes you need to access.\n",
        "\n",
        "For information on the format of the data here, see https://carbon-intensity.github.io/api-definitions/#regional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uEDgTjJFYiZ"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "def get_api_data():\n",
        "  url = \"https://api.carbonintensity.org.uk/regional\"\n",
        "  json_data = requests.get(url).json()['data']\n",
        "  # print(json_data)\n",
        "  data_table = json_data[0]\n",
        "  # print(data_table)\n",
        "  df = pd.json_normalize(data_table['regions'])\n",
        "  return df\n",
        "\n",
        "generation_df = get_api_data()\n",
        "display(generation_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ow6HQWqIsh0"
      },
      "source": [
        "### Exercise - upload a csv file to your github repository and create a data table from it\n",
        "\n",
        "Visit the Kent and Medway Air Quality site: https://kentair.org.uk/\n",
        "\n",
        "Collect a data file containing data on Ozone levels in Dover:\n",
        "\n",
        "Open the site\n",
        "Go to the Data page  \n",
        "Launch the data selector tool  \n",
        "Select:\n",
        "*  Automatic monitoring data\n",
        "*  Measurement data and simple statistics\n",
        "*  Ozone\n",
        "*  Daily mean\n",
        "*  This month\n",
        "*  Thurrock\n",
        "*  Thurrock\n",
        "\n",
        "Click on Download CSV  (This should be downloaded into your Downloads folder).\n",
        "\n",
        "**NEXT**\n",
        "\n",
        "Add the file to your Github repository. \n",
        "* rename the file dover-ozone-daily-mean.csv \n",
        "* sign in to your Github account\n",
        "* open your repository\n",
        "* click on Add a file\n",
        "* upload the air data file\n",
        "\n",
        "To be able to open the file from github, you will need to get the link to the raw file.  \n",
        "* open the file on Github\n",
        "* find the button 'Raw' and click on it\n",
        "* copy the URL    \n",
        "\n",
        "**NEXT**  \n",
        "\n",
        "Write some code to display the dataframe and compare the contents with the output on the site you took the data from.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxCrBhaWKsku"
      },
      "source": [
        "# add code here to read the csv file and display the dataframe (see above for help - From a csv hosted on Github)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - read from an Excel spreadsheet\n",
        "---\n",
        "Open the datasets list: [here](https://docs.google.com/document/d/1cijDOCDixsYu-Rr9pC8STPPXado3xoFpgBAZgdDTLHs/edit?usp=sharing)  \n",
        "\n",
        "* Find a dataset that is an Excel file\n",
        "\n",
        "* Copy the code above (for Excel files on Github) into the code cell below  \n",
        "\n",
        "* Copy the URL of the Excel file you have chosen in the datasets list  \n",
        " \n",
        "* Change the line\n",
        "```\n",
        "df = pd.read_excel(url,sheet_name=\"Industry Migration\") \n",
        "```\n",
        "to \n",
        "```\n",
        "df = pd.read_excel(url)\n",
        "```\n",
        "This will then open the first sheet in the Excel file, rather than a named sheet.\n",
        "\n",
        "* Run the code to open the data."
      ],
      "metadata": {
        "id": "iL8MgXxSZEuY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SgPuXv4caFbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "---\n",
        "*  Copy the code from the cell above -  API delivered in JSON format\n",
        "*  Change the URL to add /scotland to the end of it\n",
        "*  get the data_table as before\n",
        "*  create a new variable, **generation_data** to hold ```table_data['data']```\n",
        "*  take a look ```generation_data[0]``` by printing it\n",
        "*  normalize ```generation_data[0]```  \n",
        "*  display the resulting dataframe\n",
        "\n",
        "You will notice that there is only one row of data but that the column headed ```generationmix``` has a list of items in that one row.  \n",
        "\n",
        "You can use the ```df = df.explode('generationmix')```  \n",
        "This will expand the table to have a row for each item in the ```generationmix``` column. \n",
        "\n",
        "**Extension**:  \n",
        "\n",
        "You will notice that the first column (which is generally the index column)  has only 0s.  This is because the index for the original single row was 0 so it has kept it.  To re-index, tell the explode function to ignore the original index.  Like this:\n",
        "```\n",
        "df = df.explode('generationmix', ignore_index=True)\n",
        "```\n",
        "\n",
        "There is still a way to go to get data like this ready for use but you can start to see what can be done."
      ],
      "metadata": {
        "id": "kn-okYP4aqo8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IT-OLNOgbL0S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}